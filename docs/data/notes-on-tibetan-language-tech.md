# Notes on Tibetan Language Tech 
Preserving all these old texts might have seemed counterintuitive for a project aiming at revitalizing the publishing and freelancing industry. But in a century powered by data where knowledge and information becomes a prized commodity, BDRC’s effort in preserving and digitizing all this data starts to make a LOT of sense. The project is moving from a past-focused preservation project to a forward looking data project providing the building blocks for the creation of tomorrow’s culture. 

The Google OCR (optical character recognition) project marks the start of open big data for Tibetan.

Currently, the Tibetan language lacks three building blocks to use the language tools available to users of languages like English and Chinese.



1. A machine-readable version of the language’s complete literature
2. Standards and tools to define and segment words and sentences
3. Standards and tools to define parts of speech and other linguistic annotations

All forms of language-based artificial intelligence from machine translation and search engines to spell checkers and sentiment analysis in advertising depend on the first building block (huge amounts of data). Most AI models need to be trained with sentences and some also need to be trained with words and therefore rely on the second building block (defining words and sentences). Virtually all publication and education technology depend on all three building blocks.

The current Google OCR project of creating a machine-encoded text version of BDRC’s collection goes a long way toward creating the first building block. These sets will be available for anyone to use in the form of raw data. This will be most useful for large corporations (FAANG). We will also train some models and make some pre-trained models available. This will be useful for small and medium sized companies, as well as freelancers who have the ability to fine-tune the models.

 the first of three steps to prepare the data sets to 

Foundation models that can be fine-tuned for specific applications or domains





1. **FAANG and the like**: Out of 15 major AI applications (machine translation, speech-to-text, search engines, etc.) only these two have been developed:
    1. Google OCR is available to use for profit
    2. Microsoft Bing Translate has a Machine Translation foundation model that can be fine-tuned for profit
2. **Freelancers** can make money by fine-tune these large models (foundation models) and selling them for specific purposes (already possible with Bing Translate)
3. **Educators and researchers** can use 
1. Large datasets
    1. Raw text datasets on OpenPecha
    2. AI modelsw
2. Word and sentence segmentation standards and tools
    3. Segmented datasets
    4. Open and free tools like pybo and botok
    5. AI models
3. Linguistic annotation standards and tools
    6. Annotated datasets
    7. Annotation standard like [Universal POS tags](https://universaldependencies.org/u/pos/)
    8. Industry standard tools like [spaCy](https://spacy.io/)
    9. Datasets on tools like [SketchEngine](https://www.sketchengine.eu/)
    10. AI models

**1. Turning images into machine readable text** (image to make text machine-readable e-texts) is the first step in democratizing Tibet. It will be followed by **2) defining words and sentences**. Tibetan is a “continuous script”, with no a need a standard way to segment cut text into words. Tibetan still doesn’t have a consensus on what a word and sentence is. This has hug impact on technology. Tib can’t use a lot of tools that other languages use because of this: spell checking, analyzing sentiment, etc. All depends on these two steps.** 3) Linguistic annotations**

These three steps bring Tibetan up to speed to use tools available to other languages

Impact on two levels 



1. Outside of China big companies, FAANG and in China Waidoo, Alibaba, etc. these companies train their translation models. When these data sets are made available, these companies will use the data and improve it. Got in touch with several including google and microsoft. They said that if we have prepared data, they will incorporate it into tools. Wide reaching impact. 

    These allow search engines to analyze text to provide better **search**. They can’t search for words in Tibetan. Because of the lack of consensus about what a word is. Digital divide between English or Chinese native speakers and Tibetans is huge. Searching on Google doesn’t work for Tibetan. So they are not yet in the 21st century. Jobs, knowledge, and education is impaired because of this. A big lama said that Tibetans think that they have much less opportunity for grants, jobs, etc. But that isn’t true, they just aren’t good at looking for information. This gives them a lack of self-confidence.


    **writing**. English has auto suggestions, and grammar checkers. Without this, Tibetans are shy about writing. They think that they will make a lot of mistakes. They compare themselves to English or Chinese writers and feel a lack of confidence.


    **Teaching**. Books available at appropriate reading level. Challenging but not too challenging. By grade level Lexile (book recommendation by grade level, Scholastic. Done with AI. 


    All possible by having access to large data sets. Not possible now because Tibetan doesn’t have this. Often Tibetan children get assigned books that are too hard. Then they lose confidence and don’t write much. Their reading and writing competence is lower than speakers of other languages. Unless China tackles this, it won’t happen. But in small countries without much money, these tools have helped them. 

2.  Future proof in the next 10 years major companies make models and then smaller companies can 





* Voice-controlled assistants like Siri and Alexa.
* Natural language generation for question answering by customer service chatbots.
* Streamlining the recruiting process on sites like LinkedIn by scanning through people’s listed skills and experience.
* Tools like Grammarly which use NLP to help correct errors and make suggestions for simplifying complex writing.
* Language models like autocomplete which are trained to predict the next words in a text, based on what has already been typed.
* Google Neural Machine Translation (GNMT)
* NLP is widely used in healthcare. It is particularly useful in aggregating information from electronic health record systems, which is full of unstructured data. 
* It can help with all kinds of NLP tasks like tokenising (also known as word segmentation), part-of-speech tagging, creating text classification datasets, and much more.
* Syntax analysis
* This is often linked to sentiment analysis
* lemmatization and stemming.
* Summarisation is an NLP task that is often used in journalism and on the many newspaper sites that need to summarize news stories. 
* Named entity recognition (NER) is also used on these sites to help with tagging and displaying related stories in a hierarchical order on the web page.